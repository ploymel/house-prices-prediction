{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLtSpfRIygv3"
   },
   "source": [
    "# House Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTpjWbnAyoau"
   },
   "source": [
    "## About Completition\n",
    "### Description\n",
    "Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n",
    "\n",
    "With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n",
    "\n",
    "### Goal\n",
    "It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n",
    "\n",
    "### Evaluation\n",
    "Submissions are evaluated on **Root-Mean-Squared-Error (RMSE)** between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwnS5R6HBtUt"
   },
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ph5rfkjwzKCk"
   },
   "source": [
    "## 1. Check GPU For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1318,
     "status": "ok",
     "timestamp": 1585891514312,
     "user": {
      "displayName": "陶玫婉",
      "photoUrl": "",
      "userId": "08765119561666702662"
     },
     "user_tz": -480
    },
    "id": "F9L1uP3Gyf2l",
    "outputId": "8583deb7-ebcc-42f3-87ef-6dbfad747f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks, optimizers\n",
    "\n",
    "tf.keras.backend.clear_session() \n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hT6epUI20Aq"
   },
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bODQVp_r26Bp"
   },
   "source": [
    "### 2.1 Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDMZiDnN35gS"
   },
   "source": [
    "#### Mounted Data\n",
    "Skip this if you are using your own computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s7dXfPLF1d8g"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zR0J8tJI38hf"
   },
   "source": [
    "#### Set Path\n",
    "Change this path according to your dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYZRpSxE3yaZ"
   },
   "outputs": [],
   "source": [
    "folderPath = './'\n",
    "dataPath = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdRMKV8A9Y06"
   },
   "source": [
    "### 2.2 Pharse\n",
    "First pharse the data using `pandas`, then divide the `X_train` data into 2 groups which we've discussed in the preprocess section (for complex and deep and wide model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2702,
     "status": "ok",
     "timestamp": 1585891517661,
     "user": {
      "displayName": "陶玫婉",
      "photoUrl": "",
      "userId": "08765119561666702662"
     },
     "user_tz": -480
    },
    "id": "ajmmRzIg5PtD",
    "outputId": "c9060a17-f0c0-43bb-c74d-ce0023c44392"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1457, 219)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "X_train = pd.read_csv(dataPath + 'preprocessed_train_x.csv')\n",
    "y_train = pd.read_csv(dataPath + 'preprocessed_train_y.csv')\n",
    "\n",
    "# Remove ID\n",
    "X_train = X_train.drop('Id', axis=1)\n",
    "y_train = y_train.drop('Id', axis=1)\n",
    "\n",
    "# Devided input into 2 groups\n",
    "X_related = X_train[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']]\n",
    "X_effected = X_train.drop(['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt'], axis=1)\n",
    "\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4MDLVYvOy4S"
   },
   "source": [
    "### 2.3 Scaling Data\n",
    "In order to make it possible to train, we've used `StandardScaler` to scale down the value to make it's easily to convert while training. There are multiple type of scaler provided in sklearn. After trying multiple scaler, we'found that `StandardScaler` held the best result from our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOs5eBeDN1TV",
    "outputId": "fad6288b-2730-44a8-92ea-c67e0c611979"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "y_train = StandardScaler().fit_transform(y_train)\n",
    "X_related = StandardScaler().fit_transform(X_related)\n",
    "X_effected = StandardScaler().fit_transform(X_effected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyH9G0FU-t8k"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Histogram to check the distribution of the price after scale down\n",
    "sns.distplot(y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1U_Cwsd0Nv1c"
   },
   "source": [
    "## 3. Finding Model\n",
    "Next we tried to find the best model by conducting multiple experiments with different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hYWlGH3_wcT"
   },
   "source": [
    "### Basic Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5or-hLwWZbXD"
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(1, input_dim=X_train.shape[1], activity_regularizer=regularizers.l1(0.001)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJxqX-K4DnSR"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(layers.Dense(X_train.shape[1]//2, activation='relu'))\n",
    "    model.add(layers.Dense(1, activity_regularizer=regularizers.l1(0.001)))\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-1GAoNJD-BW"
   },
   "outputs": [],
   "source": [
    "# define wider model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(layers.Dense(1, activity_regularizer=regularizers.l1(0.001)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qlKpGnXY8YX"
   },
   "outputs": [],
   "source": [
    "def more_larger_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(200, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(layers.Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(layers.Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(layers.Dense(25, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(layers.Dense(1, kernel_initializer='normal', activity_regularizer=regularizers.l1(0.001)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSkNQU1rAA4b"
   },
   "source": [
    "### Deep and Wide Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpdO3eKr8Wx4"
   },
   "outputs": [],
   "source": [
    "def deep_and_wide_model():\n",
    "    inp = layers.Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    # Deep\n",
    "    hidden = layers.Dense(200, kernel_initializer='normal', activation='relu')(inp)\n",
    "    hidden = layers.Dense(100, kernel_initializer='normal', activation='relu')(hidden)\n",
    "    hidden = layers.Dense(50, kernel_initializer='normal', activation='relu')(hidden)\n",
    "    hidden = layers.Dense(25, kernel_initializer='normal', activation='relu')(hidden)\n",
    "    \n",
    "    # Concate\n",
    "    output = layers.concatenate([hidden, inp])\n",
    "    output = layers.Dense(200, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(100, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(50, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(25, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(1, kernel_initializer='normal', activity_regularizer=regularizers.l1(0.001))(output)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrGfbe77AH0Y"
   },
   "source": [
    "### Complex Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfaFmdF48Wx5"
   },
   "outputs": [],
   "source": [
    "def balance_complex_model():\n",
    "    input_related = layers.Input(shape=(X_related.shape[1],))\n",
    "    input_effected = layers.Input(shape=(X_effected.shape[1],))\n",
    "    \n",
    "    # Related Side\n",
    "    dense_re = layers.Dense(200, kernel_initializer='normal', activation='relu')(input_related)\n",
    "    dense_re = layers.Dense(100, kernel_initializer='normal', activation='relu')(dense_re)\n",
    "\n",
    "    # Effected Side\n",
    "    dense_eff = layers.Dense(200, kernel_initializer='normal', activation='relu')(input_effected)\n",
    "    dense_eff = layers.Dense(100, kernel_initializer='normal', activation='relu')(dense_eff)\n",
    "    \n",
    "    # Concate\n",
    "    output = layers.concatenate([dense_re, dense_eff])\n",
    "    output = layers.Dense(200, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(100, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(50, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(25, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(1, kernel_initializer='normal', activity_regularizer=regularizers.l1(0.001))(output)\n",
    "    \n",
    "    model = models.Model(inputs=[input_related, input_effected], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrjScdLY8Wx7"
   },
   "outputs": [],
   "source": [
    "def larger_complex_model():\n",
    "    input_related = layers.Input(shape=(X_related.shape[1],))\n",
    "    input_effected = layers.Input(shape=(X_effected.shape[1],))\n",
    "    \n",
    "    # Related Side\n",
    "    dense_re = layers.Dense(200, kernel_initializer='normal', activation='relu')(input_related)\n",
    "    dense_re = layers.Dense(100, kernel_initializer='normal', activation='relu')(dense_re)\n",
    "    dense_re = layers.Dense(200, kernel_initializer='normal', activation='relu')(dense_re)\n",
    "\n",
    "    # Effected Side\n",
    "    dense_eff = layers.Dense(200, kernel_initializer='normal', activation='relu')(input_effected)\n",
    "    dense_eff = layers.Dense(200, kernel_initializer='normal', activation='relu')(dense_eff)\n",
    "    \n",
    "    # Concate\n",
    "    output = layers.concatenate([dense_re, dense_eff])\n",
    "    output = layers.Dense(400, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(200, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(100, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(50, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(25, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(1, kernel_initializer='normal', activity_regularizer=regularizers.l1(0.001))(output)\n",
    "    \n",
    "    model = models.Model(inputs=[input_related, input_effected], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jh5srTzR8Wx8"
   },
   "outputs": [],
   "source": [
    "def single_side_complex_model():\n",
    "    input_related = layers.Input(shape=(X_related.shape[1],))\n",
    "    input_effected = layers.Input(shape=(X_effected.shape[1],))\n",
    "\n",
    "    # Effected Side\n",
    "    dense_eff = layers.Dense(200, kernel_initializer='normal', activation='relu')(input_effected)\n",
    "    dense_eff = layers.Dense(200, kernel_initializer='normal', activation='relu')(dense_eff)\n",
    "    \n",
    "    # Concate\n",
    "    output = layers.concatenate([input_related, dense_eff])\n",
    "    output = layers.Dense(200, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(100, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(50, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(25, kernel_initializer='normal', activation='relu')(output)\n",
    "    output = layers.Dense(1, kernel_initializer='normal', activity_regularizer=regularizers.l1(0.001))(output)\n",
    "    \n",
    "    model = models.Model(inputs=[input_related, input_effected], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9vmVoIX-z0f"
   },
   "source": [
    "## 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2OiOzyfHtvh"
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86802,
     "status": "ok",
     "timestamp": 1585887094839,
     "user": {
      "displayName": "陶玫婉",
      "photoUrl": "",
      "userId": "08765119561666702662"
     },
     "user_tz": -480
    },
    "id": "n9c97fI9nnC9",
    "outputId": "99fc77df-5b75-4232-e05a-40e378bc0757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "Val loss:  0.08198038830536686\n",
      "Val RMSE:  0.28497294\n",
      "--------------------\n",
      "Fold:  2\n",
      "Val loss:  0.09519042350249747\n",
      "Val RMSE:  0.3072554\n",
      "--------------------\n",
      "Fold:  3\n",
      "Val loss:  0.08294809908570908\n",
      "Val RMSE:  0.2867522\n",
      "--------------------\n",
      "Fold:  4\n",
      "Val loss:  0.10181516836496562\n",
      "Val RMSE:  0.3178539\n",
      "--------------------\n",
      "Fold:  5\n",
      "Val loss:  0.10353426281938848\n",
      "Val RMSE:  0.32069647\n",
      "--------------------\n",
      "Mean loss: 0.093\n",
      "Mean RMSE: 0.304\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "fold = 1\n",
    "scores = {'loss': 0, 'rmse': 0}\n",
    "for train_index, test_index in kfold.split(X_train):\n",
    "    print('Fold: ', fold)\n",
    "    X_tr, y_tr = X_train[train_index], y_train[train_index]\n",
    "    X_val, y_val = X_train[test_index], y_train[test_index]\n",
    "    \n",
    "    X_tr_re, X_tr_eff = X_related[train_index], X_effected[train_index]\n",
    "    X_val_re, X_val_eff = X_related[test_index], X_effected[test_index]\n",
    "\n",
    "    model = larger_complex_model() # The model here can be change see section 3 for more information\n",
    "    adam = optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=adam,\n",
    "              loss=tf.keras.metrics.mean_squared_error,\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    \n",
    "    # Create log directory for tensorboard\n",
    "    log_dir = os.path.join(\n",
    "        \"logs\",\n",
    "        \"fit\",\n",
    "        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    )\n",
    "    tensorboard_callback = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Note: If the model is complex or deep and wide model uncomment this 2 lines of codes\n",
    "    model.fit(x=[X_tr_re, X_tr_eff], \n",
    "          y=y_tr, \n",
    "          validation_data = ([X_val_re, X_val_eff], y_val),\n",
    "          epochs=100, verbose=0, callbacks=[tensorboard_callback], batch_size=128)\n",
    "    \n",
    "    score = model.evaluate([X_val_re, X_val_eff], y_val, verbose=0)\n",
    "\n",
    "    # Note: If the model is the basic model uncomment this 2 lines of codes\n",
    "#     model.fit(x=X_tr, \n",
    "#           y=y_tr, \n",
    "#           validation_data = (X_val, y_val),\n",
    "#           epochs=100, verbose=0, callbacks=[tensorboard_callback], batch_size=128)\n",
    "    \n",
    "#     score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print('Val loss: ', score[0])\n",
    "    print('Val RMSE: ', score[1])\n",
    "    print('-'*20)\n",
    "\n",
    "    scores['loss'] = scores['loss'] + score[0]\n",
    "    scores['rmse'] = scores['rmse'] + score[1]\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "print('Mean loss: %.3f' % (scores['loss']/5))\n",
    "print('Mean RMSE: %.3f' % (scores['rmse']/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7uhZPTsoAYlv"
   },
   "source": [
    "### Experiment Results\n",
    "Here is the list of experiments we've conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8K4DxK6DISgU"
   },
   "source": [
    "**Experiments Report**\n",
    "\n",
    "\n",
    "*5-folds cross-validation*\n",
    "- 6 Important variables + baseline: RMSE = 0.549\n",
    "- 6 Important variables + larger: RMSE = 0.517\n",
    "- 6 Important variables + wider: RMSE = 0.383\n",
    "- 6 Important variables + more_larger: RMSE = 0.387\n",
    "\n",
    "\n",
    "- 6 Important variables + more_larger + tanh: RMSE = 0.411\n",
    "- 6 Important variables + more_larger + leakyReLU(0.1): RMSE = 0.387\n",
    "- 6 Important variables + more_larger + leakyReLU(0.3): RMSE = 0.391\n",
    "- 6 Important variables + more_larger + elu: RMSE = 0.392\n",
    "\n",
    "\n",
    "- other variables + more_larger + relu: RMSE = 0.367\n",
    "- other variables + more_larger + tanh: RMSE = 0.399\n",
    "- other variables + more_larger + elu: RMSE = 0.428\n",
    "\n",
    "\n",
    "- all variables + baseline: RMSE = 0.559\n",
    "- all variables + larger: RMSE = 0.367\n",
    "- all variables + wider: RMSE = 0.480\n",
    "- all variables + more_larger: RMSE = 0.328\n",
    "\n",
    "\n",
    "**Deep and Wide**\n",
    "- small: RMSE = 0.393\n",
    "- large: RMSE = 0.315\n",
    "\n",
    "\n",
    "**Complex**\n",
    "- balance: RMSE = 0.329\n",
    "- larger+all relu: RMSE = 0.315\n",
    "- larger+all relu+1layer on related side: RMSE = 0.304\n",
    "- single-side 1 layer: RMSE = 0.324\n",
    "- single-side 2 layer: RMSE = 0.316\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvnwnpdOAlrY"
   },
   "source": [
    "### Graph with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5x7tR3gS8WyA",
    "outputId": "988deab0-95d0-49eb-a38d-9d1d10af9320"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 34696), started 0:01:43 ago. (Use '!kill 34696' to kill it.)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-33cfd54c70322da6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-33cfd54c70322da6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6W-Tze5nMy_v"
   },
   "source": [
    "## For Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBUaJ_WEAwBy"
   },
   "source": [
    "### Training the Model\n",
    "Use all of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15108,
     "status": "ok",
     "timestamp": 1585889926054,
     "user": {
      "displayName": "陶玫婉",
      "photoUrl": "",
      "userId": "08765119561666702662"
     },
     "user_tz": -480
    },
    "id": "HdvyvsWhixbK",
    "outputId": "4a238ec8-323b-43f4-c647-5f314f97d087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_35\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_71 (InputLayer)           [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_385 (Dense)               (None, 200)          1400        input_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_72 (InputLayer)           [(None, 213)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_386 (Dense)               (None, 100)          20100       dense_385[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_388 (Dense)               (None, 200)          42800       input_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_387 (Dense)               (None, 200)          20200       dense_386[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_389 (Dense)               (None, 200)          40200       dense_388[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 400)          0           dense_387[0][0]                  \n",
      "                                                                 dense_389[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_390 (Dense)               (None, 400)          160400      concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_391 (Dense)               (None, 200)          80200       dense_390[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_392 (Dense)               (None, 100)          20100       dense_391[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_393 (Dense)               (None, 50)           5050        dense_392[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_394 (Dense)               (None, 25)           1275        dense_393[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_395 (Dense)               (None, 1)            26          dense_394[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 391,751\n",
      "Trainable params: 391,751\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1457 samples\n",
      "Epoch 1/100\n",
      "1457/1457 [==============================] - 0s 299us/sample - loss: 0.9465 - rmse: 0.9729\n",
      "Epoch 2/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.5637 - rmse: 0.7505\n",
      "Epoch 3/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.1770 - rmse: 0.4198\n",
      "Epoch 4/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.1028 - rmse: 0.3194\n",
      "Epoch 5/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0733 - rmse: 0.2693\n",
      "Epoch 6/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0613 - rmse: 0.2461\n",
      "Epoch 7/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0506 - rmse: 0.2232\n",
      "Epoch 8/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0492 - rmse: 0.2201\n",
      "Epoch 9/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0506 - rmse: 0.2233\n",
      "Epoch 10/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0428 - rmse: 0.2050\n",
      "Epoch 11/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0361 - rmse: 0.1879\n",
      "Epoch 12/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0349 - rmse: 0.1847\n",
      "Epoch 13/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0270 - rmse: 0.1618\n",
      "Epoch 14/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0257 - rmse: 0.1581\n",
      "Epoch 15/100\n",
      "1457/1457 [==============================] - 0s 35us/sample - loss: 0.0225 - rmse: 0.1474\n",
      "Epoch 16/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0198 - rmse: 0.1378\n",
      "Epoch 17/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0191 - rmse: 0.1354\n",
      "Epoch 18/100\n",
      "1457/1457 [==============================] - 0s 35us/sample - loss: 0.0177 - rmse: 0.1300\n",
      "Epoch 19/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0162 - rmse: 0.1244\n",
      "Epoch 20/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0190 - rmse: 0.1350\n",
      "Epoch 21/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0163 - rmse: 0.1246\n",
      "Epoch 22/100\n",
      "1457/1457 [==============================] - ETA: 0s - loss: 0.0106 - rmse: 0.09 - 0s 33us/sample - loss: 0.0121 - rmse: 0.1063\n",
      "Epoch 23/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0105 - rmse: 0.0989\n",
      "Epoch 24/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0101 - rmse: 0.0963\n",
      "Epoch 25/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0095 - rmse: 0.0932\n",
      "Epoch 26/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0085 - rmse: 0.0878\n",
      "Epoch 27/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0076 - rmse: 0.0828\n",
      "Epoch 28/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0065 - rmse: 0.0756\n",
      "Epoch 29/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0070 - rmse: 0.0789\n",
      "Epoch 30/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0059 - rmse: 0.0717\n",
      "Epoch 31/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0066 - rmse: 0.0764\n",
      "Epoch 32/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0082 - rmse: 0.0860\n",
      "Epoch 33/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0099 - rmse: 0.0954\n",
      "Epoch 34/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0099 - rmse: 0.0953\n",
      "Epoch 35/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0080 - rmse: 0.0847\n",
      "Epoch 36/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0102 - rmse: 0.0970\n",
      "Epoch 37/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0066 - rmse: 0.0764\n",
      "Epoch 38/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0050 - rmse: 0.0651\n",
      "Epoch 39/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0042 - rmse: 0.0581\n",
      "Epoch 40/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0047 - rmse: 0.0623\n",
      "Epoch 41/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0040 - rmse: 0.0564\n",
      "Epoch 42/100\n",
      "1457/1457 [==============================] - 0s 30us/sample - loss: 0.0035 - rmse: 0.0527\n",
      "Epoch 43/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0033 - rmse: 0.0501\n",
      "Epoch 44/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0033 - rmse: 0.0501\n",
      "Epoch 45/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0029 - rmse: 0.0466\n",
      "Epoch 46/100\n",
      "1457/1457 [==============================] - 0s 30us/sample - loss: 0.0029 - rmse: 0.0458\n",
      "Epoch 47/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0024 - rmse: 0.0408\n",
      "Epoch 48/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0023 - rmse: 0.0389\n",
      "Epoch 49/100\n",
      "1457/1457 [==============================] - 0s 30us/sample - loss: 0.0023 - rmse: 0.0395\n",
      "Epoch 50/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0026 - rmse: 0.0424\n",
      "Epoch 51/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0023 - rmse: 0.0395\n",
      "Epoch 52/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0023 - rmse: 0.0388\n",
      "Epoch 53/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0026 - rmse: 0.0425\n",
      "Epoch 54/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0029 - rmse: 0.0459\n",
      "Epoch 55/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0037 - rmse: 0.0536\n",
      "Epoch 56/100\n",
      "1457/1457 [==============================] - ETA: 0s - loss: 0.0024 - rmse: 0.04 - 0s 33us/sample - loss: 0.0031 - rmse: 0.0480\n",
      "Epoch 57/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0027 - rmse: 0.0438\n",
      "Epoch 58/100\n",
      "1457/1457 [==============================] - 0s 35us/sample - loss: 0.0023 - rmse: 0.0391\n",
      "Epoch 59/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0025 - rmse: 0.0413\n",
      "Epoch 60/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0021 - rmse: 0.0365\n",
      "Epoch 61/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0019 - rmse: 0.0337\n",
      "Epoch 62/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0018 - rmse: 0.0320\n",
      "Epoch 63/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0019 - rmse: 0.0333\n",
      "Epoch 64/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0020 - rmse: 0.0349\n",
      "Epoch 65/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0019 - rmse: 0.0328\n",
      "Epoch 66/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0019 - rmse: 0.0332\n",
      "Epoch 67/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0017 - rmse: 0.0311\n",
      "Epoch 68/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0019 - rmse: 0.0339\n",
      "Epoch 69/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0020 - rmse: 0.0343\n",
      "Epoch 70/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0018 - rmse: 0.0321\n",
      "Epoch 71/100\n",
      "1457/1457 [==============================] - 0s 30us/sample - loss: 0.0019 - rmse: 0.0337\n",
      "Epoch 72/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0018 - rmse: 0.0313\n",
      "Epoch 73/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0016 - rmse: 0.0283\n",
      "Epoch 74/100\n",
      "1457/1457 [==============================] - ETA: 0s - loss: 0.0015 - rmse: 0.02 - 0s 31us/sample - loss: 0.0018 - rmse: 0.0322\n",
      "Epoch 75/100\n",
      "1457/1457 [==============================] - ETA: 0s - loss: 0.0016 - rmse: 0.02 - 0s 31us/sample - loss: 0.0020 - rmse: 0.0356\n",
      "Epoch 76/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0020 - rmse: 0.0357\n",
      "Epoch 77/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0019 - rmse: 0.0331\n",
      "Epoch 78/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0019 - rmse: 0.0329\n",
      "Epoch 79/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0021 - rmse: 0.0369\n",
      "Epoch 80/100\n",
      "1457/1457 [==============================] - 0s 35us/sample - loss: 0.0026 - rmse: 0.0422\n",
      "Epoch 81/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0026 - rmse: 0.0423\n",
      "Epoch 82/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0029 - rmse: 0.0458\n",
      "Epoch 83/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0043 - rmse: 0.0592\n",
      "Epoch 84/100\n",
      "1457/1457 [==============================] - ETA: 0s - loss: 0.0028 - rmse: 0.04 - 0s 33us/sample - loss: 0.0031 - rmse: 0.0487\n",
      "Epoch 85/100\n",
      "1457/1457 [==============================] - 0s 31us/sample - loss: 0.0033 - rmse: 0.0500\n",
      "Epoch 86/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0030 - rmse: 0.0470\n",
      "Epoch 87/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0026 - rmse: 0.0427\n",
      "Epoch 88/100\n",
      "1457/1457 [==============================] - 0s 36us/sample - loss: 0.0035 - rmse: 0.0525\n",
      "Epoch 89/100\n",
      "1457/1457 [==============================] - ETA: 0s - loss: 0.0036 - rmse: 0.05 - 0s 31us/sample - loss: 0.0040 - rmse: 0.0571\n",
      "Epoch 90/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0049 - rmse: 0.0645\n",
      "Epoch 91/100\n",
      "1457/1457 [==============================] - 0s 30us/sample - loss: 0.0035 - rmse: 0.0518\n",
      "Epoch 92/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0027 - rmse: 0.0443\n",
      "Epoch 93/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0024 - rmse: 0.0406\n",
      "Epoch 94/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0021 - rmse: 0.0367\n",
      "Epoch 95/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0019 - rmse: 0.0335\n",
      "Epoch 96/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0015 - rmse: 0.0278\n",
      "Epoch 97/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0015 - rmse: 0.0271\n",
      "Epoch 98/100\n",
      "1457/1457 [==============================] - 0s 32us/sample - loss: 0.0014 - rmse: 0.0241\n",
      "Epoch 99/100\n",
      "1457/1457 [==============================] - 0s 33us/sample - loss: 0.0015 - rmse: 0.0266\n",
      "Epoch 100/100\n",
      "1457/1457 [==============================] - 0s 34us/sample - loss: 0.0016 - rmse: 0.0280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2389fc7f978>"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = larger_complex_model()\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss=tf.keras.metrics.mean_squared_error,\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "model.summary()\n",
    "\n",
    "# Note: If the model is the basic model uncomment this 2 lines of codes\n",
    "# model.fit(x=X_train, \n",
    "#           y=y_train, \n",
    "#           epochs=100, batch_size=128)\n",
    "\n",
    "# Note: If the model is complex or deep and wide model uncomment this 2 lines of codes\n",
    "model.fit(x=[X_related, X_effected], \n",
    "          y=y_train, \n",
    "          epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1504,
     "status": "ok",
     "timestamp": 1585890036472,
     "user": {
      "displayName": "陶玫婉",
      "photoUrl": "",
      "userId": "08765119561666702662"
     },
     "user_tz": -480
    },
    "id": "ounSAiabjQyU",
    "outputId": "4b259e38-70e1-4032-8cb9-578416a98200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save model for further use\n",
    "model.save(folderPath + \"model/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sH5UoH08WyP"
   },
   "source": [
    "### Making a Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JcPUVZc8WyP"
   },
   "source": [
    "#### Preparing Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwVlkVMM8WyQ",
    "outputId": "c8cba73a-0e7e-4c36-cef3-1ee28756959f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ploymel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_csv(dataPath + 'preprocessed_test.csv')\n",
    "y_train = pd.read_csv(dataPath + 'preprocessed_train_y.csv')\n",
    "# Remove ID\n",
    "X_test = X_test.drop('Id', axis=1)\n",
    "y_train = y_train.drop('Id', axis=1)\n",
    "# Devided input into 2 groups\n",
    "X_related_test = X_test[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']]\n",
    "X_effected_test = X_test.drop(['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt'], axis=1)\n",
    "# Scale Data\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "X_related_test = StandardScaler().fit_transform(X_related_test)\n",
    "X_effected_test = StandardScaler().fit_transform(X_effected_test)\n",
    "scaler = StandardScaler().fit(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ff8eZj7B8WyW"
   },
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCXhguxs8WyW",
    "outputId": "57abcc71-8cd6-4ad8-8c58-d0e4c4d0fd43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict([X_related_test, X_effected_test])\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQUI4ezt8WyX"
   },
   "source": [
    "Inverse the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ6mx5SS8WyX"
   },
   "outputs": [],
   "source": [
    "# Inverse transformation\n",
    "result = scaler.inverse_transform(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3AeiBVO8WyZ"
   },
   "source": [
    "### Saving to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhdGz6so8WyZ"
   },
   "outputs": [],
   "source": [
    "def save_to_file(result, filename='submission.csv'):\n",
    "    row_id = 1461\n",
    "    results = {'Id': [], 'SalePrice': []}\n",
    "    for r in result:\n",
    "        price = np.exp(r[0])\n",
    "        results['Id'].append(row_id)\n",
    "        results['SalePrice'].append(price)\n",
    "        row_id += 1\n",
    "\n",
    "    df = pd.DataFrame(data=results)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_A3ZgYBy8Wya"
   },
   "outputs": [],
   "source": [
    "save_to_file(result, filename=folderPath+'submission.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8hYWlGH3_wcT",
    "XSkNQU1rAA4b",
    "GrGfbe77AH0Y"
   ],
   "name": "train.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
